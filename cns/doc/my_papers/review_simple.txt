Машинное обучение спайковых нейронных сетей.

1. Нейронные модели
1.1. Нейрон Маккалока-Питса
   Описание стандартного нейрона, описание того, что этот нейрон по сути является  приближением к спайковому нейрону

1.2. Модель Ходжкина-Хаксли
   Классическая первая модель ноб. Лауреатов, раскрывает всю сложность настоящего нейрона, моделирование занимает много TFlops и, как следствие, больших сетей из такого нейрона не построишь, хотя в вычислительной нейронауке эта модель тщательно исследована 
   
1.3. Модель Integrate-and-Fire
Упрощение модели Ходжкина-Хаксли, до динамической системы с одной переменной, проще моделировать, но, такая модель повторяет только части динамики биологических нейронов.

1.4. Модель Ижикевича
Модель нейрона в виде динамической системы с двумя переменными, довольно проста и в то же время имеет богатую динамику. Модель является компромиссом между упрощенной моделью IaF и HH.

1.5. Spike Response Model
Отдельным рядом стоит модель SRM, в своём оригинальном виде модель повторяет IaF, но в своей формулировке наиболее удобна для теоретического исследования. Наиболее часто эту модель используют со стохастическим порогом, который позволяет процесс генерации спайка описать негомогенным пуассоновским процессом.

1.6. Spike Response Model с адаптацией
Усложнение модели SRM, которая повторяет феномен адаптации.

2. Обучение с учителем и без учителя
2.1. Классическое правило Хэбба
2.2. Обучение на основе градиента ошибки
2.3. Обучение на основе феноменологической модели STDP
2.4. Теоретическая оптимальная модель STDP
3. Обучение с подкреплением
3.1. Трехфакторное правило обучения
3.2. Гедонистический синапс
3.3. Обучение на основе TD-ошибки
4. Выводы
Использованная литература








4.1. Нейрон Маккалока-Питса



















































1. Нейронные модели
1.1. Нейрон МакКаллока-Питтса
   
   Первая модель нейрона, положившая начало нейроинформатике  - модель МакКаллока-Питтса. Эта модель прочно заложила фундамент теории нейронных сетей, и исследования новых свойств этой модели не прекращаются по сей день.
   Впервые, была реализована идея использовать нейрон, как вычислительный элемент. Раннее развитие данного направления в основном характеризуется попыткой рассмотреть нейроны, как элементы, реализующие простейшие логические операции или преобразования.
   Ключевой особенностью данной модели является то, что нейрон представляется как взвешенный сумматор входных признаков
   Не смотря на ошеломляющий успех и широкое применения данной модели 
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

1.2. Модель Ходжкина-Хаксли
   Классическая первая модель ноб. Лауреатов, раскрывает всю сложность настоящего нейрона, моделирование занимает много TFlops и, как следствие, больших сетей из такого нейрона не построишь, хотя в вычислительной нейронауке эта модель тщательно исследована 
   
1.3. Модель Integrate-and-Fire
Упрощение модели Ходжкина-Хаксли, до динамической системы с одной переменной, проще моделировать, но, такая модель повторяет только части динамики биологических нейронов.

1.4. Модель Ижикевича
Модель нейрона в виде динамической системы с двумя переменными, довольно проста и в то же время имеет богатую динамику. Модель является компромиссом между упрощенной моделью IaF и HH.

1.5. Spike Response Model
Отдельным рядом стоит модель SRM, в своём оригинальном виде модель повторяет IaF, но в своей формулировке наиболее удобна для теоретического исследования. Наиболее часто эту модель используют со стохастическим порогом, который позволяет процесс генерации спайка описать негомогенным пуассоновским процессом.

1.6. Spike Response Model с адаптацией
Усложнение модели SRM, которая повторяет феномен адаптации.

2. Обучение с учителем и без учителя
2.1. Классическое правило Хэбба
2.2. Обучение на основе градиента ошибки
2.3. Обучение на основе феноменологической модели STDP
2.4. Теоретическая оптимальная модель STDP
3. Обучение с подкреплением
3.1. Трехфакторное правило обучения
3.2. Гедонистический синапс
3.3. Обучение на основе TD-ошибки
4. Выводы
Использованная литература







































