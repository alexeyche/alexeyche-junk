\documentclass[a4paper,10pt]{article}
\usepackage[T2A]{fontenc}
\usepackage[utf8x]{inputenc}
\usepackage{ucs}
\usepackage{cmap}
\usepackage[english,russian]{babel}
\usepackage{amsmath}
\usepackage{color,graphicx}
\title{Заголовок}
\author{Автор}
\begin{document}
\setlength{\parindent}{10pt}
\section*{Вводные слова}
Основная задача будущего исследования - создать унифицированный подход в использовании возможностей сложных динамических нейроннных систем в проблемах обработки больших объемов данных и осуществления при помощи них классических задач машинного обучения таких как кластеризация, регрессия, классификация.\\
\indent Динамические системы вдохновленные структурой коры головного мозга давно являются, как минимум, любопытным феноменом для учёных. Мозг является сложным многосистемным вычислительным устройством, который поражает своими вычислительными способностями и подчерпнуть хоть немного из его строения, научиться это использовать, является большим вызовом.
\paragraph*{Нейронные сети.} Первое поколение Искусственных Нейронных Сетей (ИНС) взяло только основные принципы работы биологических нейросетей. В них нейрон формализован как сумматор принимающий на вход сигналы от других нейронов и на основе искусственно введенной нелинейности решает передавать сигнал дальше или нет.\\
\indent Вторым поколением ИНС можно назвать модели школы Хинтона\cite{hinton2006}. Эти модели имеют стохастическую природу - нейрон передает или не передает сигнал в зависимости от вероятности выражаемой через коэффициенты модели, которые обучаются на данных без учителя.\\ 
\indent Обучение без учителя является ключевым показателем интеллектуальности алгоритма. Интеллект сам по себе, как свойство живых организмов, представляет собой следствие самоорганизации природы в определенных условиях\cite{evolut}. Руководствуясь подобными предпосылками, на базе школы Хинтона выросло направление в \textit{Machine Learning - Representation Learning}\cite{yoshua}.\\
\indent \textit{Representation Learning} собрал для себя следующие принципы, которые, по большей части, взяты именно из особенностей работы мозга: 
\begin{itemize}
\item распределенность хранения (distributed representation);
\item разреженность представления (sparse code);
\item генеративные модели (generative models);
\end{itemize}
Общий тренд развития интеллектуальных алгоритмов стремительно идёт по направлению к биоподобным моделям.
\paragraph*{Паралелли с Neuroscience.} Neuroscience, как наука, за последние 50 лет накопила множество опыта в исследовании особенностей функционирования мозга. Computational Neuroscience серьезно продвинулся в моделировании биологических процессов, начиная со знаменитой работы Hodkin и Huxley\cite{hodhux} в моделировании нейрона кальмара, заканчивая масштабными симуляциями нейронной активности\cite{izh_large} (порядка $10^4$ нейронов).\\
\indent В отличии от Neuroscience данная работа не ставит целью воспроизвести биологические процессы, но создать подход в применении таких моделей для конкретных задач.
\section*{Предпосылки и мотивация применения биологических принципов}
В данном разделе рассмотрены наиболее перспективные направления развития интеллектуальных алгоритмов и систем по пути биологического моделирования. В статье Randal O'Reilly \cite{randall98} описываются принципы построения когнитивных моделей, которые ещё не получили широкого распространения.
\begin{enumerate}
\item \textit{Биологический реализм.} \\
Повторение биологических принципов и применение их в моделях поможет лучше понять и использовать когнитивные способности организмов. Важным является не полагаться полностью на них, ввиду большой сложности биохимических реакций, но уметь выставить баланс между реализмом и функциональной пользой этого реализма. \\
\indent \\
\textbf{Архитектурные принципы.}
\item \textit{Распределенное представление.} \\
Биологические нейронные системы широко используют понятие распределенного разреженного представления информации. Каждый нейрон является вычислительным элементом активность которого представляет собой микропризнак (\textit{microfeature}). Различные сочетания нейронной активности представляют ряд выделенных признаков которыми кодируется информация. Функциональная польза такого подхода заключается в большой эффективности, робастности, точности представления и возможности создавать связи подобия \cite{hinton86}. Принцип подобия исходит из наличия общих элементов представляющих признаки какой либо информации. \\ 
В качестве простого примера распределенности можно рассмотреть алфавит. Каждая буква алфавита является признаком, различные небольшие сочетания признаков (слова) могут закодировать огромное количество информации.
\item \textit{Конкуренция тормозящих связей.}\\
Конкуренция тормозящих связей создаёт селективность когнитивной модели в выборе репрезентаций и как следствие формирует фундамент для обучения модели. Конкуренция заключается в погашении слабой активности, так, что только небольшой процент самых активных нейронов остается представлять данный квант информации.\\
Большинство обучающих механизмов построено по подобным принципам --  конкуренция элементов представлять информацию с течением времени приводит к тому, что наиболее важная информация <<схватывается>>   элементами.
\item \textit{Рекуррентные потоки активности.}\\
Рекуррентный обмен активностью внутри модели контрастирует на фоне классических сетей прямого распространения и являет собой важный механизм итеративного процесса представления информации определенным паттерном по достижению системы состояния эквилибриума (баланса).\\
Важность интерактивности для понимания когнитивных процессов была показана в работе McClelland и Rumelhart\cite{mcclel_rumer}. В этой работе было показано, что интерактивность может объяснить влияние высокоуровневого слоя обработки слов на низкоуровневые слои обработки букв.  \\
\indent \\
\item \textit{Обучение.}
\section*{Наиболее перспективные направления моделирования нейронных сетей.}
Здесь будет dendritic computation, reservoir computing и/или polychronius groups, time encoding, inh synaptic plasctisity 

\section*{Конкретная модель}
\end{enumerate}

\newpage
\begin{thebibliography}{9}
\bibitem{hinton2006}
Hinton, G. E., Osindero, S. and Teh, Y. (2006)
A fast learning algorithm for deep belief nets.
Neural Computation, 18, pp 1527-1554
\bibitem{evolut}
Christoph Adami, Charles Ofria, and Travis C. Collier, Evolution of biological complexity (1997), PNAS 2000 97 (9) 4463-4468; doi:10.1073/pnas.97.9.4463
\bibitem{yoshua}
Yoshua Bengio, Aaron Courville, Pascal Vincent (2012), Representation Learning: A Review and New Perspectives, CoRR abs/1206.5538 
\bibitem{hodhux}
A. L. Hodgkin. A. F. Huxley. Chance and design in electrophysiology: an informal account of certain experiments on nerve carried out between 1934 and 1952. J Physiol, 263(1):1-21, Dec 1976.
\bibitem{izh_large}
E. M. Izhikevich and G. M. Edelman. The Neurosciences Institute,10640 John Jay Hopkins Drive, San Diego, CA, 92121.
\bibitem{randall98}
Randall C. O'Reilly, Six Principles for Biologically-Based Computational Models of Cortical Cognition, TRENDS IN COGNITIVE SCIENCES, 1998, 455--462
\bibitem{hinton86}
Hinton, G. E., McClelland, J. L., and Rumelhart, D. E. (1986)
Distributed representations. 
In Rumelhart, D. E. and McClelland, J. L., editors, Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 1: Foundations, MIT Press, Cambridge, MA.
\bibitem{mcclel_rumer}
McClelland, J. L., and Rumelhart, D. E. (1981). An interactive activation model of context effects in letter perception: Part 1. An account of basic findings. Psychological Review, 88, 375-407.
\end{thebibliography}

\end{document}
