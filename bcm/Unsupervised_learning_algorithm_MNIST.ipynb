{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code illustrates the fast AI implementation of the unsupervised \"biological\" learning algorithm from [Unsupervised Learning by Competing Hidden Units](https://doi.org/10.1073/pnas.1820458116) on MNIST data set. \n",
    "If you want to learn more about this work you can also check out this [lecture](https://www.youtube.com/watch?v=4lY-oAY0aQU) from MIT's [6.S191 course](http://introtodeeplearning.com/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell loads the data and normalizes it to the [0,1] range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "mat = scipy.io.loadmat('mnist_all.mat')\n",
    "\n",
    "Nc=10\n",
    "N=784\n",
    "Ns=60000\n",
    "\n",
    "mat = scipy.io.loadmat('mnist_all.mat')\n",
    "\n",
    "def read_mnist(mat, label, num_classes=10):\n",
    "    X = np.zeros((0, 784))\n",
    "    Y = np.zeros((0, num_classes))\n",
    "    for i in range(num_classes):\n",
    "        data = mat[label + str(i)] # [:500]\n",
    "\n",
    "        X = np.concatenate((X, data), axis=0)\n",
    "\n",
    "        Yi = np.zeros((data.shape[0], num_classes))\n",
    "        Yi[:, i] = 1.0\n",
    "        Y = np.concatenate((Y, Yi), axis=0)\n",
    "\n",
    "    X = X / 255.0\n",
    "    return X.astype(np.float32), Y.astype(np.float32)\n",
    "\n",
    "M, MY = read_mnist(mat, \"train\", Nc)\n",
    "Mt, MYt = read_mnist(mat, \"test\", Nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, optimizers, Sequential, metrics, callbacks\n",
    "\n",
    "def train_nn(\n",
    "    X, \n",
    "    Y, \n",
    "    Xt, \n",
    "    Yt, \n",
    "    hidden_layer_size=None, \n",
    "    epochs=300,\n",
    "    init_learning_rate=0.02,\n",
    "    verbose=0\n",
    "):\n",
    "    def read_mnist_tf(X, Y):\n",
    "        return (\n",
    "            tf.data.Dataset.from_tensor_slices((X,Y))\n",
    "                .shuffle(X.shape[0])\n",
    "                .batch(100)\n",
    "        )\n",
    "    \n",
    "    def scheduler(epoch):\n",
    "        return init_learning_rate * (1.0 - epoch / epochs)\n",
    "        \n",
    "    train_dataset = read_mnist_tf(X, Y)\n",
    "    test_dataset = read_mnist_tf(Xt, Yt)\n",
    "    \n",
    "    if hidden_layer_size is not None:\n",
    "        model = Sequential([\n",
    "            layers.Dense(hidden_layer_size, activation='relu'), \n",
    "            layers.Dense(Y.shape[1])\n",
    "        ])\n",
    "    else:\n",
    "        model = Sequential([\n",
    "            layers.Dense(Y.shape[1])\n",
    "        ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizers.SGD(init_learning_rate),\n",
    "        loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.fit(\n",
    "        train_dataset, epochs=epochs,\n",
    "        validation_data=test_dataset,\n",
    "        validation_steps=2,\n",
    "        callbacks=[\n",
    "            callbacks.EarlyStopping(monitor='val_loss', patience=10),\n",
    "            callbacks.LearningRateScheduler(scheduler)\n",
    "        ],\n",
    "        verbose=verbose\n",
    "    )    \n",
    "    return max(model.history.history[\"val_accuracy\"])\n",
    "\n",
    "def draw_weights(synapses, Kx, Ky):\n",
    "    yy=0\n",
    "    HM=np.zeros((28*Ky,28*Kx))\n",
    "    for y in range(Ky):\n",
    "        for x in range(Kx):\n",
    "            HM[y*28:(y+1)*28,x*28:(x+1)*28]=synapses[yy,:].reshape(28,28)\n",
    "            yy += 1\n",
    "    plt.clf()\n",
    "    nc=np.amax(np.absolute(HM))\n",
    "    im=plt.imshow(HM,cmap='bwr',vmin=-nc,vmax=nc)\n",
    "    fig.colorbar(im,ticks=[np.amin(HM), 0, np.amax(HM)])\n",
    "    plt.axis('off')\n",
    "    fig.canvas.draw()   \n",
    "    \n",
    "def weights_inner_product(W, p):\n",
    "    return np.asarray([np.linalg.norm(W[:, ni], ord=p) for ni in range(W.shape[1])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines paramaters of the algorithm: `eps0` - initial learning rate that is linearly annealed during training; `hid` - number of hidden units that are displayed as an `Ky` by `Kx` array by the helper function defined above; `mu` - the mean of the gaussian distribution that initializes the weights; `sigma` - the standard deviation of that gaussian; `Nep` - number of epochs; `Num` - size of the minibatch; `prec` - parameter that controls numerical precision of the weight updates; `delta` - the strength of the anti-hebbian learning; `p` - Lebesgue norm of the weights; `k` - ranking parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps0=2e-2    # learning rate\n",
    "Kx=45        # so number of hidden units around 2000 (2025)\n",
    "Ky=45\n",
    "hid=Kx*Ky    # number of hidden units that are displayed in Ky by Kx array\n",
    "mu=0.0\n",
    "sigma=1.0\n",
    "Nep=200      # number of epochs\n",
    "Num=101      # size of the minibatch\n",
    "prec=1e-30\n",
    "delta=0.4    # Strength of the anti-hebbian learning\n",
    "p=2.0        # Lebesgue norm of the weights\n",
    "k=2          # ranking parameter, must be integer that is bigger or equal than 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline performance of 1-layer NN configuration\n",
    "\n",
    "Let's learn NN with MNIST data with just 1-layer NN configration to see the peformance of a network w/o any additional transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.9088 - accuracy: 0.7899 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/300\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.5159 - accuracy: 0.8710 - val_loss: 0.3880 - val_accuracy: 0.8900\n",
      "Epoch 3/300\n",
      "600/600 [==============================] - 3s 6ms/step - loss: 0.4463 - accuracy: 0.8828 - val_loss: 0.4106 - val_accuracy: 0.8800\n",
      "Epoch 4/300\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.4121 - accuracy: 0.8892 - val_loss: 0.4139 - val_accuracy: 0.8750\n",
      "Epoch 5/300\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.3906 - accuracy: 0.8938 - val_loss: 0.3901 - val_accuracy: 0.9000\n",
      "Epoch 6/300\n",
      "600/600 [==============================] - 3s 6ms/step - loss: 0.3761 - accuracy: 0.8972 - val_loss: 0.3130 - val_accuracy: 0.9150\n",
      "Epoch 7/300\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.3650 - accuracy: 0.8994 - val_loss: 0.3656 - val_accuracy: 0.9000\n",
      "Epoch 8/300\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.3561 - accuracy: 0.9014 - val_loss: 0.3550 - val_accuracy: 0.8750\n",
      "Epoch 9/300\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.3488 - accuracy: 0.9035 - val_loss: 0.2720 - val_accuracy: 0.9300\n",
      "Epoch 10/300\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.3428 - accuracy: 0.9048 - val_loss: 0.3356 - val_accuracy: 0.9000\n",
      "Epoch 11/300\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.3377 - accuracy: 0.9054 - val_loss: 0.2600 - val_accuracy: 0.9300\n",
      "Epoch 12/300\n",
      "600/600 [==============================] - 4s 6ms/step - loss: 0.3331 - accuracy: 0.9069 - val_loss: 0.3019 - val_accuracy: 0.9250\n",
      "Epoch 13/300\n",
      "600/600 [==============================] - 3s 6ms/step - loss: 0.3294 - accuracy: 0.9078 - val_loss: 0.3148 - val_accuracy: 0.9200\n",
      "Epoch 14/300\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.3260 - accuracy: 0.9087 - val_loss: 0.3369 - val_accuracy: 0.9250\n",
      "Epoch 15/300\n",
      "600/600 [==============================] - 3s 6ms/step - loss: 0.3227 - accuracy: 0.9100 - val_loss: 0.3303 - val_accuracy: 0.9200\n",
      "Epoch 16/300\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.3199 - accuracy: 0.9106 - val_loss: 0.3360 - val_accuracy: 0.9150\n",
      "Epoch 17/300\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.3173 - accuracy: 0.9115 - val_loss: 0.2825 - val_accuracy: 0.9200\n",
      "Epoch 18/300\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.3151 - accuracy: 0.9123 - val_loss: 0.2984 - val_accuracy: 0.9200\n",
      "Epoch 19/300\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.3131 - accuracy: 0.9131 - val_loss: 0.3879 - val_accuracy: 0.8950\n",
      "Epoch 20/300\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.3106 - accuracy: 0.9134 - val_loss: 0.3416 - val_accuracy: 0.9150\n",
      "Epoch 21/300\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.3088 - accuracy: 0.9139 - val_loss: 0.2537 - val_accuracy: 0.9400\n",
      "Epoch 22/300\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.3071 - accuracy: 0.9144 - val_loss: 0.3145 - val_accuracy: 0.9150\n",
      "Epoch 23/300\n",
      "600/600 [==============================] - 3s 6ms/step - loss: 0.3055 - accuracy: 0.9150 - val_loss: 0.2682 - val_accuracy: 0.9400\n",
      "Epoch 24/300\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.3040 - accuracy: 0.9153 - val_loss: 0.2442 - val_accuracy: 0.9350\n",
      "Epoch 25/300\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.3027 - accuracy: 0.9160 - val_loss: 0.4823 - val_accuracy: 0.8900\n",
      "Epoch 26/300\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.3014 - accuracy: 0.9161 - val_loss: 0.3002 - val_accuracy: 0.9200\n",
      "Epoch 27/300\n",
      "600/600 [==============================] - 3s 6ms/step - loss: 0.3002 - accuracy: 0.9162 - val_loss: 0.2546 - val_accuracy: 0.9250\n",
      "Epoch 28/300\n",
      "600/600 [==============================] - 3s 6ms/step - loss: 0.2991 - accuracy: 0.9170 - val_loss: 0.4024 - val_accuracy: 0.8900\n",
      "Epoch 29/300\n",
      "600/600 [==============================] - 3s 6ms/step - loss: 0.2979 - accuracy: 0.9168 - val_loss: 0.3104 - val_accuracy: 0.9150\n",
      "Epoch 30/300\n",
      "600/600 [==============================] - 3s 6ms/step - loss: 0.2969 - accuracy: 0.9169 - val_loss: 0.2603 - val_accuracy: 0.9300\n",
      "Epoch 31/300\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.2959 - accuracy: 0.9176 - val_loss: 0.1715 - val_accuracy: 0.9600\n",
      "Epoch 32/300\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.2950 - accuracy: 0.9176 - val_loss: 0.2550 - val_accuracy: 0.9150\n",
      "Epoch 33/300\n",
      "600/600 [==============================] - 3s 6ms/step - loss: 0.2940 - accuracy: 0.9181 - val_loss: 0.2902 - val_accuracy: 0.9300\n",
      "Epoch 34/300\n",
      "600/600 [==============================] - 3s 6ms/step - loss: 0.2931 - accuracy: 0.9186 - val_loss: 0.2880 - val_accuracy: 0.9250\n",
      "Epoch 35/300\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.2923 - accuracy: 0.9186 - val_loss: 0.2999 - val_accuracy: 0.8950\n",
      "Epoch 36/300\n",
      "600/600 [==============================] - 4s 6ms/step - loss: 0.2914 - accuracy: 0.9188 - val_loss: 0.3835 - val_accuracy: 0.8900\n",
      "Epoch 37/300\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.2907 - accuracy: 0.9192 - val_loss: 0.3059 - val_accuracy: 0.9100\n",
      "Epoch 38/300\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.2898 - accuracy: 0.9191 - val_loss: 0.2954 - val_accuracy: 0.9150\n",
      "Epoch 39/300\n",
      "600/600 [==============================] - 3s 6ms/step - loss: 0.2892 - accuracy: 0.9193 - val_loss: 0.2541 - val_accuracy: 0.9300\n",
      "Epoch 40/300\n",
      "600/600 [==============================] - 3s 6ms/step - loss: 0.2884 - accuracy: 0.9196 - val_loss: 0.2526 - val_accuracy: 0.9350\n",
      "Epoch 41/300\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.2880 - accuracy: 0.9200 - val_loss: 0.3177 - val_accuracy: 0.8850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nn(M, MY, Mt, MYt, hidden_layer_size=None, epochs=300, init_learning_rate=0.02, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline performance of multilayered NN configuration, 2025 hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.6820 - accuracy: 0.8450 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/300\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.3531 - accuracy: 0.9043 - val_loss: 0.3343 - val_accuracy: 0.9050\n",
      "Epoch 3/300\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3018 - accuracy: 0.9166 - val_loss: 0.3615 - val_accuracy: 0.9050\n",
      "Epoch 4/300\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.2725 - accuracy: 0.9246 - val_loss: 0.2827 - val_accuracy: 0.9350\n",
      "Epoch 5/300\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.2512 - accuracy: 0.9309 - val_loss: 0.2895 - val_accuracy: 0.9150\n",
      "Epoch 6/300\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 0.2338 - accuracy: 0.9358 - val_loss: 0.2356 - val_accuracy: 0.9400\n",
      "Epoch 7/300\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.2187 - accuracy: 0.9393 - val_loss: 0.1753 - val_accuracy: 0.9500\n",
      "Epoch 8/300\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.2062 - accuracy: 0.9431 - val_loss: 0.1309 - val_accuracy: 0.9750\n",
      "Epoch 9/300\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.1945 - accuracy: 0.9469 - val_loss: 0.1926 - val_accuracy: 0.9250\n",
      "Epoch 10/300\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.1844 - accuracy: 0.9496 - val_loss: 0.2095 - val_accuracy: 0.9550\n",
      "Epoch 11/300\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.1753 - accuracy: 0.9517 - val_loss: 0.1905 - val_accuracy: 0.9400\n",
      "Epoch 12/300\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.1670 - accuracy: 0.9542 - val_loss: 0.1358 - val_accuracy: 0.9600\n",
      "Epoch 13/300\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.1594 - accuracy: 0.9560 - val_loss: 0.1813 - val_accuracy: 0.9350\n",
      "Epoch 14/300\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.1526 - accuracy: 0.9581 - val_loss: 0.1020 - val_accuracy: 0.9700\n",
      "Epoch 15/300\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.1464 - accuracy: 0.9603 - val_loss: 0.1085 - val_accuracy: 0.9700\n",
      "Epoch 16/300\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.1406 - accuracy: 0.9621 - val_loss: 0.1157 - val_accuracy: 0.9700\n",
      "Epoch 17/300\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.1351 - accuracy: 0.9636 - val_loss: 0.0963 - val_accuracy: 0.9600\n",
      "Epoch 18/300\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.1303 - accuracy: 0.9654 - val_loss: 0.1391 - val_accuracy: 0.9550\n",
      "Epoch 19/300\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.1257 - accuracy: 0.9664 - val_loss: 0.1043 - val_accuracy: 0.9650\n",
      "Epoch 20/300\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.1216 - accuracy: 0.9674 - val_loss: 0.1265 - val_accuracy: 0.9700\n",
      "Epoch 21/300\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.1175 - accuracy: 0.9686 - val_loss: 0.1314 - val_accuracy: 0.9750\n",
      "Epoch 22/300\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.1138 - accuracy: 0.9695 - val_loss: 0.1248 - val_accuracy: 0.9550\n",
      "Epoch 23/300\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.1103 - accuracy: 0.9706 - val_loss: 0.1696 - val_accuracy: 0.9450\n",
      "Epoch 24/300\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.1070 - accuracy: 0.9717 - val_loss: 0.1044 - val_accuracy: 0.9750\n",
      "Epoch 25/300\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 0.1038 - accuracy: 0.9724 - val_loss: 0.0862 - val_accuracy: 0.9850\n",
      "Epoch 26/300\n",
      "600/600 [==============================] - 14s 23ms/step - loss: 0.1010 - accuracy: 0.9734 - val_loss: 0.0554 - val_accuracy: 0.9800\n",
      "Epoch 27/300\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 0.0982 - accuracy: 0.9741 - val_loss: 0.1108 - val_accuracy: 0.9650\n",
      "Epoch 28/300\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.0954 - accuracy: 0.9748 - val_loss: 0.1617 - val_accuracy: 0.9650\n",
      "Epoch 29/300\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.0931 - accuracy: 0.9757 - val_loss: 0.0945 - val_accuracy: 0.9700\n",
      "Epoch 30/300\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0907 - accuracy: 0.9762 - val_loss: 0.1034 - val_accuracy: 0.9650\n",
      "Epoch 31/300\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.0885 - accuracy: 0.9768 - val_loss: 0.1062 - val_accuracy: 0.9650\n",
      "Epoch 32/300\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.0863 - accuracy: 0.9773 - val_loss: 0.0825 - val_accuracy: 0.9750\n",
      "Epoch 33/300\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0843 - accuracy: 0.9782 - val_loss: 0.1035 - val_accuracy: 0.9750\n",
      "Epoch 34/300\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.0824 - accuracy: 0.9786 - val_loss: 0.1248 - val_accuracy: 0.9650\n",
      "Epoch 35/300\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.0804 - accuracy: 0.9794 - val_loss: 0.1043 - val_accuracy: 0.9650\n",
      "Epoch 36/300\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.0786 - accuracy: 0.9798 - val_loss: 0.0989 - val_accuracy: 0.9700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.985"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nn(M, MY, Mt, MYt, hidden_layer_size=hid, epochs=300, init_learning_rate=0.02, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline of BCM network without weight changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "synapses = np.random.normal(mu, sigma, (hid, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "600/600 [==============================] - 8s 13ms/step - loss: 15.3066 - accuracy: 0.8315 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/300\n",
      "600/600 [==============================] - 5s 9ms/step - loss: 9.5309 - accuracy: 0.8616 - val_loss: 9.8231 - val_accuracy: 0.8200\n",
      "Epoch 3/300\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 8.6613 - accuracy: 0.8717 - val_loss: 13.9715 - val_accuracy: 0.8050\n",
      "Epoch 4/300\n",
      "600/600 [==============================] - 5s 9ms/step - loss: 8.7360 - accuracy: 0.8720 - val_loss: 15.7605 - val_accuracy: 0.7950\n",
      "Epoch 5/300\n",
      "600/600 [==============================] - 5s 9ms/step - loss: 8.7254 - accuracy: 0.8741 - val_loss: 9.9259 - val_accuracy: 0.9050\n",
      "Epoch 6/300\n",
      "600/600 [==============================] - 5s 9ms/step - loss: 8.6081 - accuracy: 0.8758 - val_loss: 3.7796 - val_accuracy: 0.9300\n",
      "Epoch 7/300\n",
      "600/600 [==============================] - 5s 9ms/step - loss: 8.0214 - accuracy: 0.8798 - val_loss: 12.7874 - val_accuracy: 0.8350\n",
      "Epoch 8/300\n",
      "600/600 [==============================] - 5s 9ms/step - loss: 7.7819 - accuracy: 0.8802 - val_loss: 2.0298 - val_accuracy: 0.9300\n",
      "Epoch 9/300\n",
      "600/600 [==============================] - 5s 9ms/step - loss: 7.7830 - accuracy: 0.8784 - val_loss: 4.3792 - val_accuracy: 0.9050\n",
      "Epoch 10/300\n",
      "600/600 [==============================] - 5s 9ms/step - loss: 8.0869 - accuracy: 0.8796 - val_loss: 5.9088 - val_accuracy: 0.9050\n",
      "Epoch 11/300\n",
      "600/600 [==============================] - 6s 10ms/step - loss: 7.5628 - accuracy: 0.8822 - val_loss: 7.3543 - val_accuracy: 0.8950\n",
      "Epoch 12/300\n",
      "600/600 [==============================] - 7s 11ms/step - loss: 7.4989 - accuracy: 0.8808 - val_loss: 3.6075 - val_accuracy: 0.9200\n",
      "Epoch 13/300\n",
      "600/600 [==============================] - 5s 9ms/step - loss: 7.9383 - accuracy: 0.8816 - val_loss: 5.3637 - val_accuracy: 0.8900\n",
      "Epoch 14/300\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 7.8759 - accuracy: 0.8824 - val_loss: 9.7975 - val_accuracy: 0.8750\n",
      "Epoch 15/300\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 7.4350 - accuracy: 0.8822 - val_loss: 8.8279 - val_accuracy: 0.9050\n",
      "Epoch 16/300\n",
      "600/600 [==============================] - 6s 10ms/step - loss: 7.3107 - accuracy: 0.8843 - val_loss: 4.1589 - val_accuracy: 0.8650\n",
      "Epoch 17/300\n",
      "600/600 [==============================] - 5s 9ms/step - loss: 7.2714 - accuracy: 0.8821 - val_loss: 7.9198 - val_accuracy: 0.8750\n",
      "Epoch 18/300\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 7.2268 - accuracy: 0.8850 - val_loss: 4.9260 - val_accuracy: 0.9200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.93"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ma = np.zeros((M.shape[0], hid), dtype=np.float32)\n",
    "Mta = np.zeros((Mt.shape[0], hid), dtype=np.float32)\n",
    "\n",
    "for i in range(Ns // Num):\n",
    "    inputs=np.transpose(M[i*Num:(i+1)*Num,:])\n",
    "    sig=np.sign(synapses)\n",
    "    tot_input=np.dot(sig*np.absolute(synapses)**(p-1),inputs)\n",
    "    Ma[i*Num:(i+1)*Num,:] = tot_input[:].T\n",
    "\n",
    "for i in range(Mt.shape[0] // Num):\n",
    "    inputs=np.transpose(Mt[i*Num:(i+1)*Num,:])\n",
    "    sig=np.sign(synapses)\n",
    "    tot_input=np.dot(sig*np.absolute(synapses)**(p-1),inputs)\n",
    "    Mta[i*Num:(i+1)*Num,:] = tot_input[:].T\n",
    "\n",
    "train_nn(Ma, MY, Mta, MYt, verbose=1)  # no hidden layer by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines the main code. The external loop runs over epochs `nep`, the internal loop runs over minibatches. For every minibatch the overlap with the data `tot_input` is calculated for each data point and each hidden unit. The sorted strengths of the activations are stored in `y`. The variable `yl` stores the activations of the post synaptic cells - it is denoted by g(Q) in Eq 3 of [Unsupervised Learning by Competing Hidden Units](https://doi.org/10.1073/pnas.1820458116), see also Eq 9 and Eq 10. The variable `ds` is the right hand side of Eq 3. The weights are updated after each minibatch in a way so that the largest update is equal to the learning rate `eps` at that epoch. The weights are displayed by the helper function after each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, min(W) = -5.1240, max(W) = 4.6549, |W| = 26.9375, acc = 0.9250\n",
      "Epoch 2, min(W) = -5.1285, max(W) = 4.6594, |W| = 24.3657, acc = 0.9350\n",
      "Epoch 4, min(W) = -5.0237, max(W) = 4.6204, |W| = 21.6753, acc = 0.9150\n",
      "Epoch 6, min(W) = -4.6638, max(W) = 4.5443, |W| = 19.0511, acc = 0.9300\n",
      "Epoch 8, min(W) = -4.5552, max(W) = 4.2885, |W| = 16.5440, acc = 0.9250\n",
      "Epoch 10, min(W) = -4.1431, max(W) = 3.8233, |W| = 14.1823, acc = 0.9250\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-2ea999f44a22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mxx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtot_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msynapses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mnc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mtile\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/bcm/lib/python3.7/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mtile\u001b[0;34m(A, reps)\u001b[0m\n\u001b[1;32m   1240\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdim_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnrep\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m                 \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m//=\u001b[0m \u001b[0mdim_in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for nep in range(Nep):\n",
    "    eps=eps0*(1-nep/Nep)\n",
    "    perm = np.random.permutation(Ns)\n",
    "    M=M[perm,:]\n",
    "    MY=MY[perm,:]\n",
    "    \n",
    "    for i in range(Ns // Num):\n",
    "        inputs=np.transpose(M[i*Num:(i+1)*Num,:])\n",
    "        sig=np.sign(synapses)\n",
    "        tot_input=np.dot(sig*np.absolute(synapses)**(p-1),inputs)\n",
    "        \n",
    "        y=np.argsort(tot_input,axis=0)\n",
    "        yl=np.zeros((hid,Num))\n",
    "        yl[y[hid-1,:],np.arange(Num)]=1.0\n",
    "        yl[y[hid-k],np.arange(Num)]=-delta\n",
    "        \n",
    "        xx=np.sum(np.multiply(yl,tot_input),1)\n",
    "        ds=np.dot(yl,np.transpose(inputs)) - np.multiply(np.tile(xx.reshape(xx.shape[0],1),(1,N)),synapses)\n",
    "        \n",
    "        nc=np.amax(np.absolute(ds))\n",
    "        if nc<prec:\n",
    "            nc=prec\n",
    "        synapses += eps*np.true_divide(ds,nc)\n",
    "        Ma[i*Num:(i+1)*Num,:] = tot_input[:].T\n",
    "        \n",
    "        \n",
    "    for i in range(Mt.shape[0] // Num):\n",
    "        inputs=np.transpose(Mt[i*Num:(i+1)*Num,:])\n",
    "        sig=np.sign(synapses)\n",
    "        tot_input=np.dot(sig*np.absolute(synapses)**(p-1),inputs)\n",
    "        Mta[i*Num:(i+1)*Num,:] = tot_input[:].T\n",
    "\n",
    "    if nep % (Nep // 100) == 0 or nep == Nep-1:        \n",
    "        res = train_nn(Ma, MY, Mta, MYt)\n",
    "        print(\"Epoch {}, min(W) = {:.4f}, max(W) = {:.4f}, |W| = {:.4f}, acc = {:.4f}\".format(\n",
    "            nep, \n",
    "            np.min(synapses),\n",
    "            np.max(synapses),\n",
    "            np.mean(weights_inner_product(synapses.T, 2)),\n",
    "            res\n",
    "        ))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
