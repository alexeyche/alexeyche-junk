{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.io\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, optimizers, Sequential, metrics, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_binary(x):\n",
    "    return np.where(x > 0, 1.0, 0.0).astype(np.uint8)\n",
    "        \n",
    "def relu(x):\n",
    "    return np.where(x > 0, x, 0.0)\n",
    "\n",
    "def top_k_ids(D, k):\n",
    "    Di = np.argpartition(D, -k, axis=1)\n",
    "    if k < 0:\n",
    "        Di_top = Di[:, :(-k)]\n",
    "    else:\n",
    "        Di_top = Di[:, (-k):]\n",
    "    Dv_top = np.take_along_axis(D, Di_top, axis=1)\n",
    "    sorted_top = np.argsort(Dv_top, axis=1)\n",
    "    return np.take_along_axis(\n",
    "        Di_top, \n",
    "        sorted_top if k < 0 else np.flip(sorted_top, axis=1), \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "def query_coded(xc, yc, k):\n",
    "    D = np.asarray([np.sum(ycv ^ xc, axis=1) for ycv in yc])\n",
    "    return top_k_ids(D, k)\n",
    "\n",
    "def overlaps(X, Y):\n",
    "    assert X.shape == Y.shape\n",
    "    return np.asarray([len(np.intersect1d(x, y)) for x, y in zip(X, Y)])\n",
    "\n",
    "def shm(*matrices, **kwargs):\n",
    "    plt.figure(1, figsize=(25,10))\n",
    "    for m_id, matrix in enumerate(matrices):\n",
    "        plt.subplot(len(matrices), 1, m_id+1)\n",
    "        plt.imshow(np.squeeze(matrix).T, cmap='gray', origin='lower')\n",
    "        plt.colorbar()\n",
    "\n",
    "    if kwargs.get(\"file\"):\n",
    "        plt.savefig(kwargs[\"file\"])\n",
    "        plt.clf()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def quality(proposed, gt, number_of_neighbors):\n",
    "    return np.mean(overlaps(proposed, gt) / number_of_neighbors)\n",
    "\n",
    "def quality_coded(xc, yc, gt, k):\n",
    "    return quality(query_coded(xc, yc, k), gt, k)\n",
    "\n",
    "def norm(w, factor=1.0):\n",
    "    for i in range(w.shape[1]):\n",
    "        w[:, i] = factor * w[:, i] / np.sqrt(np.sum(np.square(w[:,i])))\n",
    "        \n",
    "def plot_weights(W):\n",
    "    input_d = int(np.sqrt(W.shape[0]))\n",
    "    hidden_d = int(np.sqrt(W.shape[1]))\n",
    "\n",
    "    W_plot = np.zeros((input_d * hidden_d, input_d * hidden_d))\n",
    "    i = 0\n",
    "    for y in range(hidden_d):\n",
    "        for x in range(hidden_d):\n",
    "            W_plot[\n",
    "                y*input_d:(y+1)*input_d,\n",
    "                x*input_d:(x+1)*input_d\n",
    "            ] = W[:, i].reshape(28,28)\n",
    "            i += 1\n",
    "    return shm(W_plot)\n",
    "\n",
    "def weights_inner_product(W, p):\n",
    "    return np.asarray([np.linalg.norm(W[:, ni], ord=p) for ni in range(W.shape[1])])\n",
    "\n",
    "def train_mlp(\n",
    "    X, \n",
    "    Y, \n",
    "    Xt, \n",
    "    Yt, \n",
    "    hidden_layer_size=None, \n",
    "    epochs=300,\n",
    "    init_learning_rate=0.02,\n",
    "    verbose=0\n",
    "):\n",
    "    def read_mnist_tf(X, Y):\n",
    "        return (\n",
    "            tf.data.Dataset.from_tensor_slices((X,Y))\n",
    "                .shuffle(X.shape[0])\n",
    "                .batch(100)\n",
    "        )\n",
    "    \n",
    "    def scheduler(epoch):\n",
    "        return init_learning_rate * (1.0 - epoch / epochs)\n",
    "        \n",
    "    train_dataset = read_mnist_tf(X, Y)\n",
    "    test_dataset = read_mnist_tf(Xt, Yt)\n",
    "    \n",
    "    if hidden_layer_size is not None:\n",
    "        model = Sequential([\n",
    "            layers.Dense(hidden_layer_size, activation='relu'), \n",
    "            layers.Dense(Y.shape[1])\n",
    "        ])\n",
    "    else:\n",
    "        model = Sequential([\n",
    "            layers.Dense(Y.shape[1])\n",
    "        ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizers.SGD(init_learning_rate),\n",
    "        loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.fit(\n",
    "        train_dataset, epochs=epochs,\n",
    "        validation_data=test_dataset,\n",
    "        validation_steps=2,\n",
    "        callbacks=[\n",
    "            callbacks.EarlyStopping(monitor='val_loss', patience=10),\n",
    "            callbacks.LearningRateScheduler(scheduler)\n",
    "        ],\n",
    "        verbose=verbose\n",
    "    )    \n",
    "    return max(model.history.history[\"val_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('mnist_all.mat')\n",
    "\n",
    "def read_mnist(mat, label):\n",
    "    num_classes = 10 # 5\n",
    "    input_dim = 784\n",
    "\n",
    "    X = np.zeros((0, input_dim))\n",
    "    Y = np.zeros((0, num_classes))\n",
    "    for i in range(num_classes):\n",
    "        data = mat[label + str(i)] # [:500]\n",
    "\n",
    "        X = np.concatenate((X, data), axis=0)\n",
    "\n",
    "        Yi = np.zeros((data.shape[0], num_classes))\n",
    "        Yi[:, i] = 1.0\n",
    "        Y = np.concatenate((Y, Yi), axis=0)\n",
    "\n",
    "    X = X / 255.0\n",
    "    return X.astype(np.float32), Y.astype(np.float32)\n",
    "\n",
    "X, Y = read_mnist(mat, \"train\")\n",
    "Xt, Yt = read_mnist(mat, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.6917 - accuracy: 0.8447 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/300\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.3539 - accuracy: 0.9049 - val_loss: 0.2416 - val_accuracy: 0.9250\n",
      "Epoch 3/300\n",
      "600/600 [==============================] - 14s 23ms/step - loss: 0.3015 - accuracy: 0.9172 - val_loss: 0.2383 - val_accuracy: 0.9400\n",
      "Epoch 4/300\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.2716 - accuracy: 0.9251 - val_loss: 0.3104 - val_accuracy: 0.9300\n",
      "Epoch 5/300\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.2500 - accuracy: 0.9309 - val_loss: 0.3062 - val_accuracy: 0.9150\n",
      "Epoch 6/300\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.2325 - accuracy: 0.9359 - val_loss: 0.2129 - val_accuracy: 0.9300\n",
      "Epoch 7/300\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.2178 - accuracy: 0.9402 - val_loss: 0.1896 - val_accuracy: 0.9600\n",
      "Epoch 8/300\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.2053 - accuracy: 0.9433 - val_loss: 0.2279 - val_accuracy: 0.9200\n",
      "Epoch 9/300\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.1940 - accuracy: 0.9464 - val_loss: 0.1118 - val_accuracy: 0.9750\n",
      "Epoch 10/300\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.1839 - accuracy: 0.9494 - val_loss: 0.1680 - val_accuracy: 0.9650\n",
      "Epoch 11/300\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.1749 - accuracy: 0.9519 - val_loss: 0.1532 - val_accuracy: 0.9700\n",
      "Epoch 12/300\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.1666 - accuracy: 0.9543 - val_loss: 0.1541 - val_accuracy: 0.9650\n",
      "Epoch 13/300\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.1592 - accuracy: 0.9560 - val_loss: 0.1182 - val_accuracy: 0.9600\n",
      "Epoch 14/300\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.1524 - accuracy: 0.9579 - val_loss: 0.1393 - val_accuracy: 0.9550\n",
      "Epoch 15/300\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.1461 - accuracy: 0.9600 - val_loss: 0.0576 - val_accuracy: 0.9850\n",
      "Epoch 16/300\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.1404 - accuracy: 0.9615 - val_loss: 0.1876 - val_accuracy: 0.9500\n",
      "Epoch 17/300\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.1353 - accuracy: 0.9634 - val_loss: 0.1822 - val_accuracy: 0.9450\n",
      "Epoch 18/300\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.1302 - accuracy: 0.9650 - val_loss: 0.1520 - val_accuracy: 0.9500\n",
      "Epoch 19/300\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.1257 - accuracy: 0.9661 - val_loss: 0.1726 - val_accuracy: 0.9550\n",
      "Epoch 20/300\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.1214 - accuracy: 0.9675 - val_loss: 0.0947 - val_accuracy: 0.9650\n",
      "Epoch 21/300\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.1173 - accuracy: 0.9683 - val_loss: 0.1170 - val_accuracy: 0.9750\n",
      "Epoch 22/300\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.1137 - accuracy: 0.9694 - val_loss: 0.1340 - val_accuracy: 0.9650\n",
      "Epoch 23/300\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.1101 - accuracy: 0.9708 - val_loss: 0.1234 - val_accuracy: 0.9700\n",
      "Epoch 24/300\n",
      "600/600 [==============================] - 16s 27ms/step - loss: 0.1069 - accuracy: 0.9715 - val_loss: 0.0664 - val_accuracy: 0.9850\n",
      "Epoch 25/300\n",
      "600/600 [==============================] - 19s 31ms/step - loss: 0.1038 - accuracy: 0.9722 - val_loss: 0.1659 - val_accuracy: 0.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.985"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mlp(X, Y, Xt, Yt, hidden_layer_size=2000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_learning_rate = 0.02\n",
    "# init_learning_rate = 0.05\n",
    "num_hidden = 2000\n",
    "batch_size = 200\n",
    "prec = 1e-30\n",
    "delta = 0.4\n",
    "p = 2.0\n",
    "k = 2\n",
    "batch_ids = np.arange(batch_size)\n",
    "number_of_train_batches = X.shape[0] // batch_size\n",
    "number_of_test_batches = Xt.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, |dW| = 13121.6921, min(W) = -4.5353, max(W) = 5.1656, |W| = 27.2882, acc = 0.9800\n",
      "Epoch 5, |dW| = 5921.7600, min(W) = -4.4206, max(W) = 5.1656, |W| = 21.9708, acc = 0.9850\n",
      "Epoch 10, |dW| = 3555.7579, min(W) = -4.1871, max(W) = 5.1784, |W| = 16.5983, acc = 0.9850\n",
      "Epoch 15, |dW| = 2014.4766, min(W) = -4.2032, max(W) = 4.7444, |W| = 11.6244, acc = 0.9900\n",
      "Epoch 20, |dW| = 1016.9236, min(W) = -3.6790, max(W) = 3.8893, |W| = 7.2762, acc = 0.9700\n",
      "Epoch 25, |dW| = 442.6131, min(W) = -3.3000, max(W) = 3.3747, |W| = 3.9761, acc = 0.9600\n",
      "Epoch 30, |dW| = 176.6922, min(W) = -3.3607, max(W) = 2.7104, |W| = 2.1757, acc = 0.9500\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "W = np.random.normal(0.0, 1.0, (X.shape[1], num_hidden))\n",
    "\n",
    "Xa = np.zeros((X.shape[0], num_hidden), dtype=np.float32)\n",
    "Xta = np.zeros((Xt.shape[0], num_hidden), dtype=np.float32)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    permute_ids = np.random.permutation(X.shape[0])\n",
    "    X = X[permute_ids, :]\n",
    "    Y = Y[permute_ids, :]\n",
    "    \n",
    "    learning_rate = init_learning_rate * (1-epoch / epochs)\n",
    "\n",
    "    dW_norm = 0.0\n",
    "    for i in range(number_of_train_batches):\n",
    "        x = X[i*batch_size:(i+1)*batch_size,:]\n",
    "        \n",
    "        a = relu(np.dot(x, W) ** (p-1))\n",
    "        \n",
    "        top_ids = top_k_ids(a, k)\n",
    "\n",
    "        a_deriv = np.zeros((batch_size, num_hidden))\n",
    "        a_deriv[batch_ids, top_ids[:, 0]] = 1.0\n",
    "        a_deriv[batch_ids, top_ids[:, k-1]] = -delta\n",
    "\n",
    "        a_deriv_sum = np.sum(a_deriv * a, 0)\n",
    "\n",
    "        dW = np.dot(x.T, a_deriv) - np.expand_dims(a_deriv_sum, 0) * W\n",
    "\n",
    "        denom = np.amax(np.absolute(dW))\n",
    "\n",
    "        W += learning_rate * dW / np.where(denom > 1e-20, denom, 1e-20)\n",
    "        dW_norm += np.linalg.norm(dW)\n",
    "\n",
    "        Xa[i*batch_size:(i+1)*batch_size,:] = a[:]\n",
    "        \n",
    "    for i in range(number_of_test_batches):\n",
    "        xt = Xt[i*batch_size:(i+1)*batch_size,:]\n",
    "        \n",
    "        at = relu(np.dot(xt, W) ** (p-1))\n",
    "        \n",
    "        Xta[i*batch_size:(i+1)*batch_size,:] = at[:]\n",
    "    \n",
    "    if (epoch % (epochs // 100)) == 0 or epoch == epochs-1:\n",
    "        res = train_mlp(Xa, Y, Xta, Yt)\n",
    "        print(\"Epoch {}, |dW| = {:.4f}, min(W) = {:.4f}, max(W) = {:.4f}, |W| = {:.4f}, acc = {:.4f}\".format(\n",
    "            epoch, \n",
    "            dW_norm / number_of_train_batches,\n",
    "            np.min(W),\n",
    "            np.max(W),\n",
    "            np.mean(weights_inner_product(W, 2)),\n",
    "            res\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weights(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
