{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'raise', 'over': 'raise', 'under': 'raise', 'invalid': 'raise'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "np.seterr('raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_FIG_SIZE = (15, 10)\n",
    "\n",
    "def plot_wrapper(fn):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        ncols = kwargs.get(\"ncols\", 1)\n",
    "        id = kwargs.get(\"id\", 0)\n",
    "        nrows = len(args)\n",
    "\n",
    "        if id == 0:\n",
    "            plt.figure(figsize=kwargs.get(\"figsize\", DEFAULT_FIG_SIZE))\n",
    "\n",
    "        for a_id, a in enumerate(args):\n",
    "            plt.subplot(nrows, ncols, nrows*id + a_id+1)\n",
    "            fn(a, **kwargs)\n",
    "\n",
    "        if kwargs.get(\"file\"):\n",
    "            plt.savefig(kwargs[\"file\"])\n",
    "            plt.clf()\n",
    "        elif kwargs.get(\"show\", True) and id+1 == ncols:\n",
    "            plt.show()\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "\n",
    "@plot_wrapper\n",
    "def shm(matrix, **kwargs):\n",
    "    plt.imshow(np.squeeze(matrix).T, cmap='gray', origin='lower')\n",
    "    plt.colorbar()\n",
    "    \n",
    "def shl(*vector, **kwargs):\n",
    "    plt.figure(figsize=kwargs.get(\"figsize\", DEFAULT_FIG_SIZE))\n",
    "    \n",
    "    labels = kwargs.get(\"labels\", [])\n",
    "    for id, v in enumerate(vector):\n",
    "        if len(labels) > 0:\n",
    "            plt.plot(np.squeeze(v), label=labels[id])\n",
    "        else:\n",
    "            plt.plot(np.squeeze(v))\n",
    "\n",
    "    if len(labels) > 0:\n",
    "        plt.legend()\n",
    "    \n",
    "    if not kwargs.get(\"title\") is None:\n",
    "        plt.suptitle(kwargs[\"title\"])\n",
    "\n",
    "    if kwargs.get(\"file\"):\n",
    "        plt.savefig(kwargs[\"file\"])\n",
    "        plt.clf()\n",
    "    elif kwargs.get(\"show\", True):\n",
    "        plt.show()\n",
    "\n",
    "relu = lambda x: np.maximum(x, 0.0)\n",
    "relu_prime = lambda x: np.where(x > 0.0, 1.0, 0.0)\n",
    "\n",
    "sigmoid = lambda x: 1.0/(1.0 + np.exp(-x))\n",
    "def sigmoid_prime(x):\n",
    "    v = sigmoid(x)\n",
    "    return v * (1.0 - v)\n",
    "\n",
    "t_clip_value = 1.0\n",
    "\n",
    "threshold = lambda x: np.where(x > 0.0, 1.0, 0.0)\n",
    "threshold_prime = lambda x: np.where(relu(x) <= t_clip_value, 1.0, 0.0)\n",
    "\n",
    "# def threshold_prime(x, threshold_value = 0.0):\n",
    "#     return 1.0/np.square(1.0 + np.abs(x - threshold_value))\n",
    "\n",
    "def one_hot_encode(target_v, size=None):\n",
    "    y_v = np.zeros((target_v.shape[0], size if not size is None else len(np.unique(target_v))))\n",
    "    for cl_id, cl_v in enumerate(np.unique(target_v)):\n",
    "        y_v[np.where(target_v==cl_v)[0], cl_id] = 1.0\n",
    "\n",
    "    return y_v\n",
    "\n",
    "def weights_init(fan_in, fan_out, const=1.0):\n",
    "    low = -const * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    high = const * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return (\n",
    "        (low + np.random.random((fan_in, fan_out)) * (high - low)).astype(np.float32),\n",
    "        (low + np.random.random((fan_out,)) * (high - low)).astype(np.float32)\n",
    "    )\n",
    "\n",
    "def number_of_equal_act(a, a_t):\n",
    "    a_t_m = a_t.copy()\n",
    "    a_t_m[np.where(np.abs(a_t_m) < 1e-10)] = -1\n",
    "\n",
    "    a_m = a.copy()\n",
    "    a_m[np.where(np.abs(a_m) < 1e-10)] = -10\n",
    "\n",
    "    a_t_mean = np.mean(a_t)\n",
    "    if a_t_mean == 0.0:\n",
    "        if np.mean(np.equal(a_t_m, a_m)) == 0.0:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "    else:\n",
    "        return np.mean(np.equal(a_t_m, a_m)) / np.mean(a_t)\n",
    "\n",
    "    \n",
    "def feedback_filter(y, y_fb, ltp=1.0, ltd=-1.0):\n",
    "    return y * np.where(y == y_fb, ltp, ltd)\n",
    "#     return y * (y_fb - 1.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(1)\n",
    "\n",
    "input_dim = 20\n",
    "output_dim = 3\n",
    "batch_size = 100\n",
    "epochs = 500\n",
    "learning_rate = 0.01\n",
    "\n",
    "# f, f_prime = sigmoid, sigmoid_prime\n",
    "f, f_prime = threshold, threshold_prime\n",
    "\n",
    "W_orig, _ = weights_init(input_dim, output_dim)\n",
    "\n",
    "X = (np.random.random((batch_size, input_dim)) < 0.1).astype(np.float32)\n",
    "Yt = f(np.dot(X, W_orig))\n",
    "\n",
    "W, _ = weights_init(input_dim, output_dim)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    u = np.dot(X, W)\n",
    "    y = f(u)\n",
    "    \n",
    "    e = Yt - y\n",
    "    \n",
    "    dW = np.dot(X.T, e)\n",
    "    \n",
    "    W += learning_rate * dW\n",
    "    \n",
    "    if (epoch % (epochs // 10)) == 0 or epoch == 0 or epoch == epochs - 1:\n",
    "        print(\"Epoch {}, {:.3f}\".format(epoch, np.linalg.norm(e)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(1)\n",
    "\n",
    "input_dim = 2\n",
    "output_dim = 2\n",
    "batch_size = 4\n",
    "epochs = 500\n",
    "learning_rate = 0.01\n",
    "\n",
    "# f, f_prime = sigmoid, sigmoid_prime\n",
    "f, f_prime = threshold, threshold_prime\n",
    "\n",
    "X = np.asarray([\n",
    "    [0.0, 0.0],\n",
    "    [0.0, 1.0],\n",
    "    [1.0, 0.0],\n",
    "    [1.0, 1.0]\n",
    "], dtype=np.float32)\n",
    "Yt = one_hot_encode(np.asarray([\n",
    "    [0.0],\n",
    "    [1.0],\n",
    "    [1.0],\n",
    "    [0.0]\n",
    "], dtype=np.float32), 2)\n",
    "\n",
    "\n",
    "W, _ = weights_init(input_dim, output_dim)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    u = np.dot(X, W)\n",
    "    y = f(u)\n",
    "    \n",
    "    e = Yt - y\n",
    "    \n",
    "    dW = np.dot(X.T, e)\n",
    "    \n",
    "    W += learning_rate * dW\n",
    "    \n",
    "    if (epoch % (epochs // 10)) == 0 or epoch == 0 or epoch == epochs - 1:\n",
    "        print(\"Epoch {}, {:.3f}\".format(epoch, np.linalg.norm(e)))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, |e| 24.160 |et| 20.445 |du0| 83.561 |num.eq| 82.474%\n",
      "Epoch 100, |e| 13.040 |et| 15.166 |du0| 18.751 |num.eq| 82.705%\n",
      "Epoch 200, |e| 12.153 |et| 14.765 |du0| 19.049 |num.eq| 83.660%\n",
      "Epoch 300, |e| 11.891 |et| 14.248 |du0| 19.040 |num.eq| 86.005%\n",
      "Epoch 400, |e| 11.519 |et| 14.142 |du0| 18.726 |num.eq| 85.581%\n",
      "Epoch 500, |e| 11.351 |et| 14.353 |du0| 18.942 |num.eq| 85.087%\n",
      "Epoch 600, |e| 11.173 |et| 14.526 |du0| 18.642 |num.eq| 84.433%\n",
      "Epoch 700, |e| 10.899 |et| 14.422 |du0| 18.535 |num.eq| 85.330%\n",
      "Epoch 800, |e| 10.807 |et| 13.928 |du0| 18.438 |num.eq| 87.172%\n",
      "Epoch 900, |e| 10.457 |et| 14.283 |du0| 18.476 |num.eq| 86.283%\n",
      "Epoch 999, |e| 10.497 |et| 13.784 |du0| 18.633 |num.eq| 86.541%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "# f, f_prime = sigmoid, sigmoid_prime\n",
    "f, f_prime = threshold, threshold_prime\n",
    "\n",
    "W_orig_0 = np.random.randn(20, 100)\n",
    "W_orig_1 = np.random.randn(100, 10)\n",
    "\n",
    "# train data\n",
    "X = (np.random.random((2000, 20)) < 0.1).astype(np.float32)\n",
    "Y = f(np.dot(f(np.dot(X, W_orig_0)), W_orig_1))\n",
    "\n",
    "# test data\n",
    "Xt = (np.random.random((200, 20)) < 0.1).astype(np.float32)\n",
    "Yt = f(np.dot(f(np.dot(Xt, W_orig_0)), W_orig_1))\n",
    "\n",
    "# Xt, Yt = X, Y\n",
    "\n",
    "# X = np.asarray([\n",
    "#     [0.0, 0.0],\n",
    "#     [0.0, 1.0],\n",
    "#     [1.0, 0.0],\n",
    "#     [1.0, 1.0]\n",
    "# ], dtype=np.float32)\n",
    "# Yt = one_hot_encode(np.asarray([\n",
    "#     [0.0],\n",
    "#     [1.0],\n",
    "#     [1.0],\n",
    "#     [0.0]\n",
    "# ], dtype=np.float32), 2)\n",
    "\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "output_dim = Yt.shape[1]\n",
    "batch_size = 200\n",
    "number_of_train_batches = X.shape[0] // batch_size\n",
    "number_of_test_batches = Xt.shape[0] // batch_size\n",
    "\n",
    "hidden_dim = 300\n",
    "epochs = 1000\n",
    "init_learning_rate = 0.001\n",
    "\n",
    "W0, b0 = weights_init(input_dim, hidden_dim)\n",
    "W1, b1 = weights_init(hidden_dim, output_dim)\n",
    "W0_fb, b0_fb = weights_init(output_dim, hidden_dim)\n",
    "\n",
    "W0_start = W0.copy()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    permute_ids = np.random.permutation(X.shape[0])\n",
    "    X = X[permute_ids, :]\n",
    "    Y = Y[permute_ids, :]\n",
    "\n",
    "    e_avg, et_avg = 0.0, 0.0\n",
    "    du0_avg = 0.0\n",
    "    num_eq_avg = 0.0\n",
    "    \n",
    "    learning_rate = init_learning_rate / (epoch + 1) ** 0.5\n",
    "\n",
    "    for i in range(number_of_train_batches):\n",
    "        x = X[i*batch_size:(i+1)*batch_size,:]\n",
    "        y = Y[i*batch_size:(i+1)*batch_size,:]\n",
    "\n",
    "        u0 = np.dot(x, W0)\n",
    "        y0 = f(u0)\n",
    "\n",
    "        u1 = np.dot(y0, W1)\n",
    "        y1 = f(u1)\n",
    "\n",
    "        e = y - y1\n",
    "\n",
    "        y0_fb = f(np.dot(y, W0_fb)) \n",
    "\n",
    "        du1 = e\n",
    "#         du0 = np.dot(du1, W1.T) * f_prime(u0) # BP\n",
    "#         du0 = np.dot(du1, W0_fb) * f_prime(u0) # FA\n",
    "        du0 = feedback_filter(y0, y0_fb, 0.1, -1.0) * f_prime(u0)\n",
    "\n",
    "        dW1 = np.dot(y0.T, du1)\n",
    "        db1 = np.sum(du1, 0)\n",
    "\n",
    "        dW0 = np.dot(x.T, du0)\n",
    "        db0 = np.sum(du0, 0)\n",
    "\n",
    "        W0 += learning_rate * dW0\n",
    "        b0 += learning_rate * db0\n",
    "        W1 += learning_rate * dW1\n",
    "        b1 += learning_rate * db1\n",
    "        \n",
    "        e_avg += np.linalg.norm(e)\n",
    "        du0_avg += np.linalg.norm(du0)\n",
    "\n",
    "    for i in range(number_of_test_batches):\n",
    "        xt = Xt[i*batch_size:(i+1)*batch_size,:]\n",
    "        yt = Yt[i*batch_size:(i+1)*batch_size,:]\n",
    "\n",
    "        yt0 = f(np.dot(xt, W0))\n",
    "        yt1 = f(np.dot(yt0, W1))\n",
    "\n",
    "        et = yt - yt1\n",
    "        et_avg += np.linalg.norm(et)\n",
    "        num_eq_avg += number_of_equal_act(yt, yt1)\n",
    "        \n",
    "    if (epoch % (epochs // 10)) == 0 or epoch == 0 or epoch == epochs - 1:\n",
    "        print(\"Epoch {}, |e| {:.3f} |et| {:.3f} |du0| {:.3f} |num.eq| {:.3f}%\".format(\n",
    "            epoch, \n",
    "            e_avg /  number_of_train_batches,\n",
    "            et_avg /  number_of_test_batches,\n",
    "            du0_avg /  number_of_train_batches,\n",
    "            100.0 * num_eq_avg / number_of_test_batches,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, |e| 22.299 |et| 21.024 |du0| 121.501 |du1| 33.904 |num.eq| 73.158%\n",
      "Epoch 200, |e| 16.360 |et| 18.628 |du0| 47.338 |du1| 16.572 |num.eq| 79.129%\n",
      "Epoch 400, |e| 15.976 |et| 15.362 |du0| 45.174 |du1| 16.678 |num.eq| 88.069%\n",
      "Epoch 600, |e| 15.117 |et| 17.263 |du0| 43.693 |du1| 16.666 |num.eq| 86.027%\n",
      "Epoch 800, |e| 15.838 |et| 15.780 |du0| 42.282 |du1| 16.237 |num.eq| 85.783%\n",
      "Epoch 1000, |e| 15.331 |et| 15.716 |du0| 41.458 |du1| 16.241 |num.eq| 89.892%\n",
      "Epoch 1200, |e| 14.884 |et| 18.330 |du0| 40.370 |du1| 16.159 |num.eq| 83.556%\n",
      "Epoch 1400, |e| 14.725 |et| 14.213 |du0| 40.264 |du1| 16.164 |num.eq| 91.640%\n",
      "Epoch 1600, |e| 14.918 |et| 14.697 |du0| 39.316 |du1| 16.144 |num.eq| 87.781%\n",
      "Epoch 1800, |e| 14.990 |et| 16.186 |du0| 38.781 |du1| 15.751 |num.eq| 88.223%\n",
      "Epoch 1999, |e| 15.200 |et| 13.964 |du0| 38.452 |du1| 15.829 |num.eq| 92.597%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "# f, f_prime = sigmoid, sigmoid_prime\n",
    "f, f_prime = threshold, threshold_prime\n",
    "\n",
    "W_orig_0 = np.random.randn(20, 100)\n",
    "W_orig_1 = np.random.randn(100, 100)\n",
    "W_orig_2 = np.random.randn(100, 10)\n",
    "\n",
    "# train data\n",
    "X = (np.random.random((2000, 20)) < 0.1).astype(np.float32)\n",
    "Y = f(np.dot(f(np.dot(f(np.dot(X, W_orig_0)), W_orig_1)), W_orig_2))\n",
    "\n",
    "# test data\n",
    "Xt = (np.random.random((200, 20)) < 0.1).astype(np.float32)\n",
    "Yt = f(np.dot(f(np.dot(f(np.dot(Xt, W_orig_0)), W_orig_1)), W_orig_2))\n",
    "\n",
    "# Xt, Yt = X, Y\n",
    "\n",
    "# X = np.asarray([\n",
    "#     [0.0, 0.0],\n",
    "#     [0.0, 1.0],\n",
    "#     [1.0, 0.0],\n",
    "#     [1.0, 1.0]\n",
    "# ], dtype=np.float32)\n",
    "# Yt = one_hot_encode(np.asarray([\n",
    "#     [0.0],\n",
    "#     [1.0],\n",
    "#     [1.0],\n",
    "#     [0.0]\n",
    "# ], dtype=np.float32), 2)\n",
    "\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "output_dim = Yt.shape[1]\n",
    "batch_size = 200\n",
    "number_of_train_batches = X.shape[0] // batch_size\n",
    "number_of_test_batches = Xt.shape[0] // batch_size\n",
    "\n",
    "hidden_dim = 300\n",
    "epochs = 2000\n",
    "init_learning_rate = 0.0005\n",
    "\n",
    "W0, b0 = weights_init(input_dim, hidden_dim)\n",
    "W1, b1 = weights_init(hidden_dim, hidden_dim)\n",
    "W2, b2 = weights_init(hidden_dim, output_dim)\n",
    "W0_fb, b0_fb = weights_init(hidden_dim, hidden_dim)\n",
    "W1_fb, b1_fb = weights_init(output_dim, hidden_dim)\n",
    "\n",
    "W0_start = W0.copy()\n",
    "W1_start = W1.copy()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    permute_ids = np.random.permutation(X.shape[0])\n",
    "    X = X[permute_ids, :]\n",
    "    Y = Y[permute_ids, :]\n",
    "\n",
    "    e_avg, et_avg = 0.0, 0.0\n",
    "    du0_avg, du1_avg = 0.0, 0.0\n",
    "    num_eq_avg = 0.0\n",
    "\n",
    "    learning_rate = init_learning_rate / (epoch + 1) ** 0.5\n",
    "\n",
    "    for i in range(number_of_train_batches):\n",
    "        x = X[i*batch_size:(i+1)*batch_size,:]\n",
    "        y = Y[i*batch_size:(i+1)*batch_size,:]\n",
    "\n",
    "        u0 = np.dot(x, W0)\n",
    "        y0 = f(u0)\n",
    "\n",
    "        u1 = np.dot(y0, W1)\n",
    "        y1 = f(u1)\n",
    "\n",
    "        u2 = np.dot(y1, W2)\n",
    "        y2 = f(u2)\n",
    "\n",
    "        e = y - y2\n",
    "\n",
    "        y1_fb = f(np.dot(y, W1_fb)) \n",
    "        y0_fb = f(np.dot(y1_fb, W0_fb)) \n",
    "\n",
    "        du2 = e\n",
    "        # BP\n",
    "#         du1 = np.dot(du2, W2.T) * f_prime(u1)\n",
    "#         du0 = np.dot(du1, W1.T) * f_prime(u0)\n",
    "        \n",
    "        # FB\n",
    "        du1 = feedback_filter(y1, y1_fb, 1.0, -1.0) * f_prime(u1)\n",
    "        du0 = feedback_filter(y0, y0_fb, 1.0, -1.0) * f_prime(u0)\n",
    "\n",
    "        dW2 = np.dot(y1.T, du2)\n",
    "        db2 = np.sum(du2, 0)\n",
    "\n",
    "        dW1 = np.dot(y0.T, du1)\n",
    "        db1 = np.sum(du1, 0)\n",
    "\n",
    "        dW0 = np.dot(x.T, du0)\n",
    "        db0 = np.sum(du0, 0)\n",
    "\n",
    "        W0 += learning_rate * dW0\n",
    "        b0 += learning_rate * db0\n",
    "        \n",
    "        W1 += learning_rate * dW1\n",
    "        b1 += learning_rate * db1\n",
    "\n",
    "        W2 += learning_rate * dW2\n",
    "        b2 += learning_rate * db2\n",
    "        \n",
    "        e_avg += np.linalg.norm(e)\n",
    "        du0_avg += np.linalg.norm(du0)\n",
    "        du1_avg += np.linalg.norm(du1)\n",
    "        \n",
    "    for i in range(number_of_test_batches):\n",
    "        xt = Xt[i*batch_size:(i+1)*batch_size,:]\n",
    "        yt = Yt[i*batch_size:(i+1)*batch_size,:]\n",
    "\n",
    "        yt0 = f(np.dot(xt, W0))\n",
    "        yt1 = f(np.dot(yt0, W1))\n",
    "        yt2 = f(np.dot(yt1, W2))\n",
    "\n",
    "        et = yt - yt2\n",
    "        et_avg += np.linalg.norm(et)\n",
    "        num_eq_avg += number_of_equal_act(yt, yt2)\n",
    "        \n",
    "    if (epoch % (epochs // 10)) == 0 or epoch == 0 or epoch == epochs - 1:\n",
    "        print(\"Epoch {}, |e| {:.3f} |et| {:.3f} |du0| {:.3f} |du1| {:.3f} |num.eq| {:.3f}%\".format(\n",
    "            epoch, \n",
    "            e_avg /  number_of_train_batches,\n",
    "            et_avg /  number_of_test_batches,\n",
    "            du0_avg /  number_of_train_batches,\n",
    "            du1_avg /  number_of_train_batches,\n",
    "            100.0 * num_eq_avg / number_of_test_batches,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 10\n",
    "# FB filter 0.0, -1.0\n",
    "# Epoch 0, |e| 22.074 |et| 20.952 |du0| 80.796 |du1| 23.690 |num.eq| 75.992%\n",
    "# Epoch 200, |e| 16.260 |et| 17.635 |du0| 6.632 |du1| 2.803 |num.eq| 84.173%\n",
    "# Epoch 400, |e| 16.076 |et| 17.550 |du0| 4.653 |du1| 2.613 |num.eq| 86.032%\n",
    "# Epoch 600, |e| 15.545 |et| 17.833 |du0| 3.869 |du1| 1.917 |num.eq| 82.478%\n",
    "# Epoch 800, |e| 15.772 |et| 16.823 |du0| 3.316 |du1| 1.875 |num.eq| 86.242%\n",
    "# Epoch 1000, |e| 15.289 |et| 16.371 |du0| 2.616 |du1| 1.387 |num.eq| 85.658%\n",
    "# Epoch 1200, |e| 15.629 |et| 18.111 |du0| 2.382 |du1| 1.107 |num.eq| 90.652%\n",
    "# Epoch 1400, |e| 15.344 |et| 16.553 |du0| 2.185 |du1| 0.865 |num.eq| 88.565%\n",
    "# Epoch 1600, |e| 14.979 |et| 16.941 |du0| 1.691 |du1| 1.258 |num.eq| 86.947%\n",
    "# Epoch 1800, |e| 15.150 |et| 16.882 |du0| 1.710 |du1| 0.341 |num.eq| 85.341%\n",
    "# Epoch 1999, |e| 15.714 |et| 17.889 |du0| 0.939 |du1| 0.683 |num.eq| 81.586%\n",
    "\n",
    "# FB filter 0.1, -1.0 \n",
    "# Epoch 0, |e| 22.095 |et| 20.149 |du0| 81.351 |du1| 24.139 |num.eq| 75.459%\n",
    "# Epoch 200, |e| 17.560 |et| 18.947 |du0| 17.757 |du1| 6.499 |num.eq| 81.126%\n",
    "# Epoch 400, |e| 17.149 |et| 16.340 |du0| 17.361 |du1| 7.026 |num.eq| 86.762%\n",
    "# Epoch 600, |e| 16.833 |et| 16.793 |du0| 17.312 |du1| 6.853 |num.eq| 85.599%\n",
    "# Epoch 800, |e| 16.683 |et| 15.684 |du0| 17.233 |du1| 7.015 |num.eq| 90.513%\n",
    "# Epoch 1000, |e| 16.504 |et| 16.523 |du0| 17.033 |du1| 7.132 |num.eq| 86.831%\n",
    "# Epoch 1200, |e| 15.598 |et| 15.811 |du0| 16.741 |du1| 7.070 |num.eq| 89.018%\n",
    "# Epoch 1400, |e| 15.397 |et| 15.524 |du0| 16.829 |du1| 7.104 |num.eq| 85.019%\n",
    "# Epoch 1600, |e| 15.152 |et| 15.395 |du0| 16.567 |du1| 6.922 |num.eq| 88.525%\n",
    "# Epoch 1800, |e| 14.847 |et| 16.310 |du0| 16.567 |du1| 7.097 |num.eq| 88.498%\n",
    "# Epoch 1999, |e| 16.272 |et| 16.793 |du0| 16.454 |du1| 6.935 |num.eq| 85.315%\n",
    "\n",
    "# FB filter 1.0, -1.0\n",
    "# Epoch 0, |e| 22.299 |et| 21.024 |du0| 121.501 |du1| 33.904 |num.eq| 73.158%\n",
    "# Epoch 200, |e| 16.360 |et| 18.628 |du0| 47.338 |du1| 16.572 |num.eq| 79.129%\n",
    "# Epoch 400, |e| 15.976 |et| 15.362 |du0| 45.174 |du1| 16.678 |num.eq| 88.069%\n",
    "# Epoch 600, |e| 15.117 |et| 17.263 |du0| 43.693 |du1| 16.666 |num.eq| 86.027%\n",
    "# Epoch 800, |e| 15.838 |et| 15.780 |du0| 42.282 |du1| 16.237 |num.eq| 85.783%\n",
    "# Epoch 1000, |e| 15.331 |et| 15.716 |du0| 41.458 |du1| 16.241 |num.eq| 89.892%\n",
    "# Epoch 1200, |e| 14.884 |et| 18.330 |du0| 40.370 |du1| 16.159 |num.eq| 83.556%\n",
    "# Epoch 1400, |e| 14.725 |et| 14.213 |du0| 40.264 |du1| 16.164 |num.eq| 91.640%\n",
    "# Epoch 1600, |e| 14.918 |et| 14.697 |du0| 39.316 |du1| 16.144 |num.eq| 87.781%\n",
    "# Epoch 1800, |e| 14.990 |et| 16.186 |du0| 38.781 |du1| 15.751 |num.eq| 88.223%\n",
    "# Epoch 1999, |e| 15.200 |et| 13.964 |du0| 38.452 |du1| 15.829 |num.eq| 92.597%\n",
    "\n",
    "# BP\n",
    "# Epoch 0, |e| 22.037 |et| 20.976 |du0| 20.434 |du1| 22.077 |num.eq| 83.192%\n",
    "# Epoch 200, |e| 8.618 |et| 13.342 |du0| 5.416 |du1| 5.781 |num.eq| 90.918%\n",
    "# Epoch 400, |e| 7.488 |et| 12.884 |du0| 4.358 |du1| 4.595 |num.eq| 91.692%\n",
    "# Epoch 600, |e| 6.860 |et| 12.961 |du0| 3.979 |du1| 4.192 |num.eq| 92.189%\n",
    "# Epoch 800, |e| 7.450 |et| 12.610 |du0| 4.205 |du1| 4.434 |num.eq| 92.089%\n",
    "# Epoch 1000, |e| 6.760 |et| 12.884 |du0| 3.768 |du1| 3.963 |num.eq| 91.524%\n",
    "# Epoch 1200, |e| 6.537 |et| 12.530 |du0| 3.575 |du1| 3.771 |num.eq| 92.710%\n",
    "# Epoch 1400, |e| 6.038 |et| 12.410 |du0| 3.297 |du1| 3.467 |num.eq| 92.300%\n",
    "# Epoch 1600, |e| 5.915 |et| 12.410 |du0| 3.338 |du1| 3.501 |num.eq| 92.300%\n",
    "# Epoch 1800, |e| 5.491 |et| 12.083 |du0| 3.081 |du1| 3.232 |num.eq| 92.879%\n",
    "# Epoch 1999, |e| 5.489 |et| 12.166 |du0| 2.987 |du1| 3.109 |num.eq| 92.952%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
